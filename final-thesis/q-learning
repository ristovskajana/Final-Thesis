import pygame
import os
import random
import numpy as np
import matplotlib.pyplot as pl
import json

pygame.init()

clock = pygame.time.Clock()
fps = 60

screen_width = 864
screen_height = 936

screen = pygame.display.set_mode((screen_width, screen_height))
pygame.display.set_caption('Flappy Bird')

# LOADING ALL IMAGES
background_img = pygame.image.load('img/bg.png')
ground_img = pygame.image.load('img/ground.png')
button_img = pygame.image.load('img/restart.png')

def draw_text(text, font, color, x, y):
    img = font.render(text, True, color)
    screen.blit(img, (x, y))

# Number of episodes to run
num_episodes = 10000
episode = 0 

def reset_game():
    pipe_group.empty()
    flappy.rect.x = 100
    flappy.rect.y = int(screen_height/2)

    score = 0
    return score

# List to store cumulative rewards for each episode
episode_rewards = []
average_rewards = []

# Q-learning parameters
epsilon = 0.3
epsilon_decay = 0.00001
epsilon_min = 0.1
alpha = 0.7
gamma = 0.9

num_discrete_states = 250


num_actions = 2  # Two possible actions: 0 for not jumping, 1 for jumping

# Initialize Q-table
discretized_states = np.zeros(num_discrete_states)
q_table = np.zeros((num_discrete_states, num_actions), dtype= float)

combinations = []
for x in range(10):
    for y in range(25):
        combinations.append((x, y))

integer_array = list(range(250))

mapping = dict(zip(combinations, integer_array))


# Initialize episode records
episode_scores = []
episode_states = []  # Initialize episode states list
episode_actions = []
training_steps_per_episode = []
average_rewards = []
average_score = []
convergance_policy = []



def discretize_horizontal(value):
    bins1 = np.linspace(0, 400, 7)
    bins2 = np.linspace(401, 790, 3)
    if value <= 400:
        discretized_value = np.digitize(value, bins1)
    else:
        if np.digitize(value, bins2) == 1:
            discretized_value = 7
        if np.digitize(value, bins2) == 2:
            discretized_value = 8
        if np.digitize(value, bins2) == 3:
            discretized_value = 9
    return discretized_value # Subtract 1 to convert bin index to range 0-8


def discretize_vertical(value):
    bins = np.linspace(0, 1200, 25)  # Shift the bins to positive range
    shifted_value = value + 600  # Shift the value to positive range
    discretized_value = np.digitize(shifted_value, bins)
    return discretized_value - 1

def draw_pipe_line_horizontal(pipe):
    y_pos = pipe.rect.bottom + 170 # Y position of the pipe's bottom
    x_pos = pipe.rect.x

    pygame.draw.line(screen, (255, 0, 0), (x_pos, y_pos), (x_pos + 78, y_pos), 2) 

def draw_pipe_line_vertical(pipe):
    x_pos = pipe.rect.right  # X position of the pipe's left side
    y_pos = pipe.rect.bottom + 170  # Y position of the pipe's bottom
    # Draw the line to the top side of the pipe

    pygame.draw.line(screen, (255, 0, 0), (x_pos, screen_height - 168), (x_pos, y_pos), 2)  

def draw_bird_bottom(birdy):
    x_pos = birdy.rect.left  # X position of the bird's left side
    y_pos = birdy.rect.bottom  # Y position of the bird's bottom

    line_start = (x_pos, y_pos)
    line_end = (screen_width, y_pos)  # Draw the line to the end of the screen

    pygame.draw.line(screen, (255, 0, 0), line_start, line_end, 2)

# GAME VARIABLES
font = pygame.font.SysFont('Bauhaus 93', 60)
white = (255, 255, 255)
scroll_ground = 0
scroll_speed = 4
flying = False
dead = False
pipe_gap = 170
pipe_frequency = 1500
last_pipe = pygame.time.get_ticks() - pipe_frequency
score = 0
pass_pipe = False
gravity = 0.5

# Variable to store the high score so far
high_score = 0  

class Bird(pygame.sprite.Sprite):
    def __init__(self, x, y):
        pygame.sprite.Sprite.__init__(self)
        self.images = []
        self.index = 0
        self.counter = 0
        for num in range(1, 4):
            bird_img = pygame.image.load(f'img/bird{num}.png')
            self.images.append(bird_img)
        self.image = self.images[self.index]
        self.rect = self.image.get_rect()
        self.rect.center = [x, y]

        # Initialize Q-learning parameters
        self.learning_rate = alpha
        self.discount_factor = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        self.state = None
        # Initialize state
        self.last_state = None
        self.action = None
        self.last_action = None

        # Initialize velocity
        self.vel_y = 0

        self.flap_allowed = True  # New variable to control flapping behavior
        self.flap_cooldown = 2  # Cooldown period before another flap is allowed
        self.cooldown_counter = 0  # Counter to track the remaining cooldown frames


    def update(self):
        if flying == True:
            self.vel_y += gravity
            if self.vel_y > 8:
                self.vel_y = 8
            if self.rect.bottom < 768:
                self.rect.y += int(self.vel_y)
        else:
            if self.rect.y < 0:
                self.rect.y = 0

        if dead == False: 
            # Update index to animate bird
            self.counter += 1
            flap_cool_down  = 5
            if self.counter > flap_cool_down:
                self.counter = 0
                self.index += 1
                if self.index >= len(self.images):
                    self.index = 0
            self.image = self.images[self.index]

            # Rotate the bird
            self.image = pygame.transform.rotate(self.images[self.index], self.vel_y * -2)
            # self.rect = self.image.get_rect(center=(self.rect.centerx, self.rect.centery))

        self.rect.x = 100  # Set the desired X position here

    def jump(self):
        self.vel_y = -10

    def not_jump(self):
        if self.vel_y >= 8:
            self.vel_y = 8
        else: 
            self.vel_y += 0.5

    def update_state(self):
        horizontal_distance = self.horizontal_distance()
        vertical_distance = self.vertical_distance()
        # print("Horiz: ", horizontal_distance)
        # print("Vertical: ", vertical_distance)

        discrete_horizontal_distance = discretize_horizontal(horizontal_distance)
        discrete_vertical_distance = discretize_vertical(vertical_distance)

        self.state = mapping[(discrete_horizontal_distance, discrete_vertical_distance)]
        # print("State:", self.state, " horizontal: ", discrete_horizontal_distance, " verical: ", discrete_vertical_distance)

    def get_closest_lower_pipe(self):
        closest_lower_pipe = None
        closest_lower_pipe_dist = float('inf')
        for pipe in pipe_group:
            if pipe.position == 1: # retirve only the lower pipes
                if pipe.rect.right + 35> self.rect.right:
                    distance = (pipe.rect.right + 35) - self.rect.right
                    if distance < closest_lower_pipe_dist:
                        closest_lower_pipe_dist = distance
                        closest_lower_pipe = pipe
        return closest_lower_pipe

    
    def horizontal_distance(self):
        closest_lower_pipe = self.get_closest_lower_pipe()
        draw_pipe_line_vertical(closest_lower_pipe)
        if closest_lower_pipe is not None:
            x_distance = self.rect.right
            distance = (closest_lower_pipe.rect.right - x_distance) + 35
            # print("Pipes left: ", closest_lower_pipe.rect.left)
            # print("Bird right : ", self.rect.right)
            return distance
        else:
            return float('inf')


    def vertical_distance(self):
        closest_lower_pipe = self.get_closest_lower_pipe()
        draw_pipe_line_horizontal(closest_lower_pipe)
        if closest_lower_pipe is not None:
            y_distance = self.rect.bottom
            distance = (closest_lower_pipe.rect.bottom + pipe_gap) - y_distance
            # print("Bird bottom : ", self.rect.bottom)
            return distance
        else:
            return float('inf')
    
    def choose_action(self, q_table, state):
        jump_value = q_table[state][1]  # Value if we jump
        no_jump_value = q_table[state][0]   # Value if we don't jump
        if (jump_value > no_jump_value): 
            choice = 1
        else:
            choice = 0
        return choice

    def update_q_table(self, state, action, reward, next_state):
        if next_state is not None:
            max_q = max(q_table[next_state]) if q_table[next_state].size > 0 else 0
            q_table[state][action] += self.learning_rate * (reward + self.discount_factor * max_q - q_table[state][action])
        else:
            q_table[state][action] = reward
        
class Pipe(pygame.sprite.Sprite):
    def __init__(self, x, y, position):
        pygame.sprite.Sprite.__init__(self)
        self.image = pygame.image.load('img/pipe.png')
        self.rect = self.image.get_rect()
        self.passed = False  # Initialize the 'passed' attribute to False
        # position variable determines if the pipe is coming from the bottom or top
        self.position = position  

        if position == 1:
            self.image = pygame.transform.flip(self.image, False, True)
            self.rect.bottomleft = [x, y - int(pipe_gap/2)]
        if position == -1:
            self.rect.topleft = [x, y + int(pipe_gap/2)]

    def update(self):
        self.rect.x -= scroll_speed
        if self.rect.right < 0:
            self.kill()

# bird group keeps track of all the sprites added to it
bird_group = pygame.sprite.Group()

# pipe group keeps track of all the pipes added 
pipe_group = pygame.sprite.Group()

flappy = Bird(100, int(screen_height/2))

bird_group.add(flappy)


# Load Q-table if it exists
q_table = np.load('q_table.npy') if os.path.isfile('q_table.npy') else q_table

# Set the print options for numpy
np.set_printoptions(suppress=True, precision=4)

training_step = 1
avg_reward = 0
avg_score = 0

run = True
while run:
    clock.tick(fps)
    screen.blit(background_img, (0, 0))
    time_now = pygame.time.get_ticks()

    for event in pygame.event.get():
        if event.type == pygame.QUIT:
                run = False
        elif event.type == pygame.KEYDOWN:
            if event.key == pygame.K_SPACE:
                # Ignore spacebar press event
                pass

    # Update and draw pipes
    pipe_group.draw(screen)
    pipe_group.update()

    # Draw ground
    scroll_ground -= scroll_speed
    if abs(scroll_ground) > 35:
        scroll_ground = 0
    screen.blit(ground_img, (scroll_ground, 768))

    # Draw and update birds
    bird_group.draw(screen)
    draw_bird_bottom(flappy)  

    # generate new pipes
    time_now = pygame.time.get_ticks()
    if time_now - last_pipe > pipe_frequency:
        pipe_height = random.randint(-100, 100)
        bottom_pipe = Pipe(screen_width, int(screen_height/2) + pipe_height, -1)
        top_pipe = Pipe(screen_width, int(screen_height/2) + pipe_height, 1)
        pipe_group.add(bottom_pipe)
        pipe_group.add(top_pipe)
        last_pipe = time_now
    
    if not dead:
        if flappy.last_state == None:
            flappy.update()
            flappy.update_state()
            flappy.last_state = flappy.state
            flappy.last_action = 0
            reward = 0
            flappy.update_q_table(flappy.last_state, flappy.last_action, reward, flappy.state)
            action = flappy.choose_action(q_table, flappy.state)

        else:
            # print("Current state is: ", flappy.last_state, " and q values are ", q_table[flappy.last_state][0], " and ", q_table[flappy.last_state][1])
            action = flappy.choose_action(q_table, flappy.last_state)

        # Update Q-learning state and choose action
        # print("Q value for opposite action is ", q_table[flappy.last_state][abs(flappy.last_action - 1)])

        flappy.action = action
        # print("Action chosen: ", flappy.action)

        if flappy.action == 1:
            flying = True
            flappy.jump()
        else:
            flying = True
            flappy.not_jump()
        
        flappy.update()
        flappy.update_state()
        # print("Current state is: ", flappy.state, " and q value is: ", q_table[flappy.state][flappy.action])

        # Check collision with pipes
        if pygame.sprite.groupcollide(bird_group, pipe_group, False, False) or flappy.rect.top < 0 :
            flying = False  
            dead = True
            next_state = None
            reward = -1000
            flappy.update_q_table(flappy.last_state, flappy.action, -1000, next_state)
            # print("For state: ", flappy.last_state, " and action performed: ", flappy.action, " new q value is: ", q_table[flappy.last_state][flappy.action])

        # check if the bird has hit the ground
        if flappy.rect.bottom >= 768:
            dead = True
            flying = False
            next_state = None
            flappy.update_q_table(flappy.last_state, flappy.action, -1000, next_state)   
            # print("For state: ", flappy.last_state, " and action performed: ", flappy.action, " new q value is: ", q_table[flappy.last_state][flappy.action])

        if dead == False:
            reward = 10

        # Check if bird has passed the pipe
        for pipe in pipe_group:
            if pipe.rect.right < (flappy.rect.right + 35) and pipe.passed == False:
                pipe.passed = True
                pass_pipe = True

        # Update Q-table
        if dead == False:
            next_state = flappy.state  # Update next state
            flappy.update_q_table(flappy.last_state, flappy.action, reward, next_state)  # Update the Q-table using the flappy object
                # print("For state: ", flappy.last_state, " and action performed: ", flappy.action, " new q value is: ", q_table[flappy.last_state][flappy.action])

            # Update score
        if pass_pipe:
            score += 1
            pass_pipe = False

            if score > high_score:
                high_score = score

        # Update last state and last action
        flappy.last_state = flappy.state

        training_step += 1
        avg_reward += reward

        # Draw score
        draw_text(str(score), font, white, int(screen_width / 2), 20)


    # Check if the bird has hit the ground
    if dead == True:
        print(" -----------------------------New Episode--------------------------------------------")
        # print("For state: ", flappy.last_state, " and action performed: ", flappy.last_action, " new q value is: ", q_table[flappy.last_state][flappy.last_action])
        dead = False
        flying = False
        # print("Q-table:")
        # print(q_table)  # Print the Q-table
        print("Episode: ", episode + 1)
        episode_states.append(flappy.state)
        episode_actions.append(action)
        training_steps_per_episode.append(training_step)
        # average_rewards.append(avg_reward)
        # avg_score = training_step/score
        # average_score.append(avg_score)

        # Update episode rewards
        episode_scores.append(score)

        episode += 1
        print("Current Score: ", score)
        score = reset_game()
        training_step = 1
        avg_reward = 0
        time_now = pygame.time.get_ticks()
        last_pipe = time_now - 2000
        if episode >= num_episodes:
            run = False

        # Decay epsilon
        if epsilon > epsilon_min:
            epsilon -= epsilon_decay
            # print("Epsilon is: ", epsilon)


        # Save the Q-table after each episode
        np.save('q_table.npy', q_table)

        # Load Q-table for the next episode
        q_table = np.load('q_table.npy')

        # Print dimensions of the Q-table
        # print("Q-table dimensions:", q_table.shape)
        # Print high score
        print("High Score:", high_score)

    # Append current state and action to episode records
    episode_states.append(flappy.state)
    episode_actions.append(action)

    pygame.display.update()

pl.scatter(range(len(episode_scores)), episode_scores)
pl.xlabel("Training episode")
pl.ylabel("Score")
pl.title("Performance Analysis")
pl.savefig("score_plot.png")  # Save the plot as an image file
pl.show()

pl.scatter(range(len(episode_scores)), training_steps_per_episode)
pl.xlabel("Episode")
pl.ylabel("Training steps per episode")
pl.title("Training steps")
pl.savefig("training_plot.png")  # Save the plot as an image file
pl.show()

pl.scatter(training_steps_per_episode, episode_scores)
pl.xlabel("Training steps")
pl.ylabel("Score")
pl.title("Score/Training steps")
pl.savefig("score_training_plot.png")  # Save the plot as an image file
pl.show()




# Save episode rewards as a JSON file
with open("episode_scores.json", "w") as f:
    json.dump(episode_scores, f)

with open("training_steps.json", "w") as f:
    json.dump(training_steps_per_episode, f)

with open("episode_rewards.json", "w") as f:
    json.dump(episode_rewards, f)

pygame.quit()
